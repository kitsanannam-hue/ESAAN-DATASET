````
Repository: icaropires/pdf2dataset
Files analyzed: 20

Estimated tokens: 17.9k

Directory structure:
└── icaropires-pdf2dataset/
    ├── README.md
    ├── LICENSE
    ├── pyproject.toml
    ├── requirements.txt
    ├── .flake8
    ├── pdf2dataset/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── extract_task.py
    │   ├── extraction.py
    │   ├── extraction_memory.py
    │   ├── pdf_extract_task.py
    │   ├── results.py
    │   └── utils.py
    ├── tests/
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── test_extract_task.py
    │   ├── test_extraction.py
    │   ├── test_extraction_memory.py
    │   ├── testing_dataframe.py
    │   └── samples/
    │       ├── sub1/
    │       └── sub2/
    └── .github/
        └── workflows/
            └── ci.yml

================================================
FILE: README.md
================================================
# pdf2dataset

[![pdf2dataset](https://github.com/icaropires/pdf2dataset/workflows/pdf2dataset/badge.svg?branch=master)](https://github.com/icaropires/pdf2dataset)
[![pypi](https://img.shields.io/pypi/v/pdf2dataset.svg)](https://pypi.python.org/pypi/pdf2dataset)
[![Maintainability](https://api.codeclimate.com/v1/badges/cbe90c3043b038f52b18/maintainability)](https://codeclimate.com/github/icaropires/pdf2dataset/maintainability)
[![codecov](https://codecov.io/gh/icaropires/pdf2dataset/branch/master/graph/badge.svg)](https://codecov.io/gh/icaropires/pdf2dataset)
[![pypi-stats](https://img.shields.io/pypi/dm/pdf2dataset)](https://pypistats.org/packages/pdf2dataset)

Converts a whole subdirectory with any volume (small or huge) of PDF documents to a dataset (pandas DataFrame).
No need to setup any external service (no database, brokers, etc). Just install and run it!

## Main features

* Conversion of a whole subdirectory with PDFs documents into a pandas DataFrame
* Support for parallel and distributed processing through [ray](https://github.com/ray-project/ray)
* Extractions are performed by page, making tasks distribution more uniform for handling documents with big differences in number of pages
* Incremental writing of resulting DataFrame, making possible to process data bigger than memory
* Error tracking of faulty documents
* Resume interrupted processing
* Extract text through [pdftotext](https://github.com/jalan/pdftotext)
* Use OCR for extracting text through [pytesseract](https://github.com/madmaze/pytesseract)
* Extract images through [pdf2image](https://github.com/Belval/pdf2image)
* Support to implement custom features extraction
* Highly customizable behavior through params

## Installation

### Install Dependencies

#### Fedora

``` bash
# "-por" for portuguese, use the documents language
$ sudo dnf install -y gcc-c++ poppler-utils pkgconfig poppler-cpp-devel python3-devel tesseract-langpack-por
```

#### Ubuntu (or debians)

``` bash
$ sudo apt update

# "-por" for portuguese, use the documents language
$ sudo apt install -y build-essential poppler-utils libpoppler-cpp-dev pkg-config python3-dev tesseract-ocr-por
```

### Install pdf2dataset

#### For usage

``` bash
$ pip3 install pdf2dataset --user  # Please, isolate the environment
```

#### For development

``` bash
# First, install poetry, clone repository and cd into it
$ poetry install
```

## Usage

### Simple - CLI

``` bash
# Note: path, page and error will always be present in resulting DataFrame

# Reads all PDFs from my_pdfs_dir and saves the resultant dataframe to my_df.parquet.gzip
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip  # Most basic, extract all possible features
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip --features=text  # Extract just text
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip --features=image  # Extract just image
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip --num-cpus 1  # Maximum reducing of parallelism
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip --ocr true  # For scanned PDFs
$ pdf2dataset my_pdfs_dir my_df.parquet.gzip --ocr true --lang eng  # For scanned documents with english text
```

### Resume processing

In case of any interruption, to resume the processing, just use the same path as output and the
processing will be resumed automatically. The flag `--saving-interval` (or the param `saving_interval`)
controls the frequency the output path will be updated, and so, the processing "checkpoints".

### Using as a library

#### Main functions

There're some helper functions to facilitate pdf2dataset usage:

* **extract:** function can be used analogously to the CLI
* **extract_text**: `extract` wrapper with `features=text`
* **extract_image**: `extract` wrapper with `features=image`
* **image_from_bytes:** (pdf2image.utils) get a Pillow `Image` object given the image bytes
* **image_to_bytes:** (pdf2image.utils) get the image bytes given the a Pillow `Image` object

#### Basic example
``` python
from pdf2dataset import extract

extract('my_pdfs_dir', 'all_features.parquet.gzip')
```

#### Small data

One feature, not available to the CLI, is the custom behavior for handling small volumes of data (small can
be understood as that: the extraction won't run for hours or days and won't be distributed).

The complete list of differences are:

* Faster initialization (use multiprocessing instead of ray)
* Don't save processing progress
* Distributed processing not supported
* Don't write dataframe to disk
* Returns the dataframe

##### Example:
``` python
from pdf2dataset import extract_text

df = extract_text('my_pdfs_dir', small=True)
# ...
```

#### Pass list of files paths

Instead of specifying a directory, one can specify a list of files to be processed.

##### Example:

``` python
from pdf2dataset import extract

my_files = [\
    './tests/samples/single_page1.pdf',\
    './tests/samples/invalid1.pdf',\
]

df = extract(my_files, small=True)
# ...
```

#### Pass files from memory

If you don't want to specify a directory for the documents, you can specify the tasks that
will be processed.

The tasks can be of the form `(document_name, document_bytes, page_number)`
or just `(document_name, document_bytes)`, **document_name** must ends with `.pdf` but
don't need to be a real file, **document_bytes** are the bytes of the pdf document and
**page_number** is the number of the page to process (all pages, if not specified).

##### Example:

``` python
from pdf2dataset import extract_text

tasks = [\
    ('a.pdf', a_bytes),  # Processing all pages of this document\
    ('b.pdf', b_bytes, 1),\
    ('b.pdf', b_bytes, 2),\
]

# 'df' will contain results from all pages from 'a.pdf' and page 1 and 2 from 'b.pdf'
df = extract_text(tasks, 'my_df.parquet.gzip', small=True)

# ...
```

#### Returning a list

If you don't want to handle the DataFrame, is possible to return a nested list with the features values.
The structure for the resulting list is:
```
result = List[documents]
documents = List[pages]
pages = List[features]
features = List[feature]
feature = any
```

* `any` is any type supported by pyarrow.
* features are ordered by the feature name (`text`, `image`, etc)

##### Example:

``` python
>>> from pdf2dataset import extract_text
>>> extract_text('tests/samples', return_list=True)
[[[None]],\
 [['First page'], ['Second page'], ['Third page']],\
 [['My beautiful sample!']],\
 [['First page'], ['Second page'], ['Third page']],\
 [['My beautiful sample!']]]
```

* Features with error will have `None` value as result
* Here, `extract_text` was used, so the only feature is `text`

#### Custom Features

With version >= 0.4.0, is also possible to easily implement extraction of custom features:

##### Example:

This is the structure:

``` python
from pdf2dataset import extract, feature, PdfExtractTask

class MyCustomTask(PdfExtractTask):

    @feature('bool_')
    def get_is_page_even(self):
        return self.page % 2 == 0

    @feature('binary')
    def get_doc_first_bytes(self):
        return self.file_bin[:10]

    @feature('string', exceptions=[ValueError])
    def get_wrong(self):
        raise ValueError("There was a problem!")

if __name__ == '__main__':
    df = extract('tests/samples', small=True, task_class=MyCustomTask)
    print(df)

    df.dropna(subset=['text'], inplace=True)  # Discard invalid documents
    print(df.iloc[0].error)
```

* First print:
```
                         path  page doc_first_bytes  ...                  text  wrong                                              error
0                invalid1.pdf    -1   b"I'm invali"  ...                  None   None  image_original:\nTraceback (most recent call l...
1             multi_page1.pdf     2  b'%PDF-1.5\n%'  ...           Second page   None  wrong:\nTraceback (most recent call last):\n  ...
2             multi_page1.pdf     3  b'%PDF-1.5\n%'  ...            Third page   None  wrong:\nTraceback (most recent call last):\n  ...
3   sub1/copy_multi_page1.pdf     1  b'%PDF-1.5\n%'  ...            First page   None  wrong:\nTraceback (most recent call last):\n  ...
4   sub1/copy_multi_page1.pdf     3  b'%PDF-1.5\n%'  ...            Third page   None  wrong:\nTraceback (most recent call last):\n  ...
5             multi_page1.pdf     1  b'%PDF-1.5\n%'  ...            First page   None  wrong:\nTraceback (most recent call last):\n  ...
6  sub2/copy_single_page1.pdf     1  b'%PDF-1.5\n%'  ...  My beautiful sample!   None  wrong:\nTraceback (most recent call last):\n  ...
7   sub1/copy_multi_page1.pdf     2  b'%PDF-1.5\n%'  ...           Second page   None  wrong:\nTraceback (most recent call last):\n  ...
8            single_page1.pdf     1  b'%PDF-1.5\n%'  ...  My beautiful sample!   None  wrong:\nTraceback (most recent call last):\n  ...

[9 rows x 8 columns]
```

* Second print:
```
wrong:
Traceback (most recent call last):
  File "/home/icaro/Desktop/pdf2dataset/pdf2dataset/extract_task.py", line 32, in inner
    result = feature_method(*args, **kwargs)
  File "example.py", line 16, in get_wrong
    raise ValueError("There was a problem!")
ValueError: There was a problem!

```

Notes:
* `@feature` is the decorator used to define new features.
* The extraction method name must start with the prefix `get_` (avoids collisions with attribute names and increases readability)
* First argument to `@feature` must be a valid PyArrow type, complete list [here](https://arrow.apache.org/docs/python/api/datatypes.html)
* `exceptions` param specify a list of exceptions to be recorded on DataFrame, otherwise they are raised
* For this example, all available features plus the custom ones are extracted

### Results File

The resulting "file" is a directory with structure specified by dask with pyarrow engine,
it can be easily read with pandas or dask:

#### Example with pandas
``` python
>>> import pandas as pd
>>> df = pd.read_parquet('my_df.parquet.gzip', engine='pyarrow')
>>> df
                             path  page                  text                                              error
index
0                single_page1.pdf     1  My beautiful sample!
1       sub1/copy_multi_page1.pdf     2           Second page
2      sub2/copy_single_page1.pdf     1  My beautiful sample!
3       sub1/copy_multi_page1.pdf     3            Third page
4                 multi_page1.pdf     1            First page
5                 multi_page1.pdf     3            Third page
6       sub1/copy_multi_page1.pdf     1            First page
7                 multi_page1.pdf     2           Second page
0                    invalid1.pdf    -1                        Traceback (most recent call last):\n  File "/h...
```

There is no guarantee about the uniqueness or order of `index`, you might need to create a new index with
the whole data in memory.

The `-1` page number means that was not possible of even parsing the document.

### Run on a Cluster

#### Setup the Cluster

Follow ray documentation for [manual](https://docs.ray.io/en/latest/using-ray-on-a-cluster.html?setup#manual-cluster-setup) or [automatic](https://docs.ray.io/en/latest/autoscaling.html?setup#automatic-cluster-setup)
setup.

#### Run it

To go distributed you can run it just like local, but using the `--address` and `--redis-password` flags to point to your cluster ([More information](https://docs.ray.io/en/latest/multiprocessing.html)).

With version >= 0.2.0, only the head node needs to have access to the documents in disk.

### CLI Help

```
usage: pdf2dataset [-h] [--features FEATURES]
                   [--saving-interval SAVING_INTERVAL] [--ocr-lang OCR_LANG]
                   [--ocr OCR] [--chunksize CHUNKSIZE]
                   [--image-size IMAGE_SIZE] [--ocr-image-size OCR_IMAGE_SIZE]
                   [--image-format IMAGE_FORMAT] [--num-cpus NUM_CPUS]
                   [--address ADDRESS] [--dashboard-host DASHBOARD_HOST]
                   [--redis-password REDIS_PASSWORD]
                   input_dir out_file

Extract text from all PDF files in a directory

positional arguments:
  input_dir             The folder to lookup for PDF files recursively
  out_file              File to save the resultant dataframe

optional arguments:
  -h, --help            show this help message and exit
  --features FEATURES   Specify a comma separated list with the features you
                        want to extract. 'path' and 'page' will always be
                        added. Available features to add: image, page, path,
                        text Examples: '--features=text,image' or '--
                        features=all'
  --saving-interval SAVING_INTERVAL
                        Results will be persisted to results folder every
                        saving interval of pages
  --ocr-lang OCR_LANG   Tesseract language
  --ocr OCR             'pytesseract' if true, else 'pdftotext'. default:
                        false
  --chunksize CHUNKSIZE
                        Chunksize to use while processing pages, otherwise is
                        calculated
  --image-size IMAGE_SIZE
                        If adding image feature, image will be resized to this
                        size. Provide two integers separated by 'x'. Example:
                        --image-size 1000x1414
  --ocr-image-size OCR_IMAGE_SIZE
                        The height of the image OCR will be applied. Width
                        will be adjusted to keep the ratio.
  --image-format IMAGE_FORMAT
                        Format of the image generated from the PDF pages
  --num-cpus NUM_CPUS   Number of cpus to use
  --address ADDRESS     Ray address to connect
  --dashboard-host DASHBOARD_HOST
                        Which IP ray webui will try to listen on
  --redis-password REDIS_PASSWORD
                        Redis password to use to connect with ray
```

## Troubleshooting

1. **Troubles with high memory usage**

* Decrease the number of CPUs in use, reducing the level of parallelism, test it with `--num-cpus 1` flag and then increase according to your hardware.

* Use smaller chunksize, so less documents will be put in memory at once. Use `--chunksize 1` for having `1 * num_cpus` documents in memory at once.

## How to Contribute

Just open your [issues](https://github.com/icaropires/pdf2dataset/issues) and/or [pull requests](https://github.com/icaropires/pdf2dataset/pulls), all are welcome :smiley:!

================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

================================================
FILE: pyproject.toml
================================================
[tool.poetry]
name = "pdf2dataset"
version = "0.5.3"
readme = "README.md"
description = "Easily convert a subdirectory with big volume of PDF documents into a dataset, supports extracting text and images"
authors = ["Ícaro Pires <icaropsa@gmail.com>"]
license = "Apache License 2.0"
classifiers = [\
    "License :: OSI Approved :: Apache Software License",\
    "Programming Language :: Python :: 3",\
]
repository = "https://github.com/icaropires/pdf2dataset"

[tool.poetry.dependencies]
dask = {extras = ["dataframe"], version = "2.23.0"}
more-itertools = "^8.4.0"
opencv-python = "4.4.0.42"
packaging = "^20.4"
pandas = "^0.25.0"
pdf2image = "^1.13.1"
pdftotext = "2.1.5"
pyarrow = "1.0.0"
pytesseract = "0.3.5"
ray = "0.8.7"
tqdm = "^4.41.0"

python = "^3.6"

[tool.poetry.dev-dependencies]
pytest = "^5.2"
flake8 = "3.8.3"
pytest-cov = "2.10.1"

[tool.poetry.scripts]
pdf2dataset = "pdf2dataset.__main__:main"

[build-system]
requires = ["poetry>=1.0.10"]
build-backend = "poetry.masonry.api"

================================================
FILE: requirements.txt
================================================
dask[dataframe]==2.23.0
flake8==3.8.3
more-itertools>=8.4.0
opencv-python==4.4.0.42
packaging>=20.4
pandas>=0.25.0
pdf2image>=1.13.1
pdftotext==2.1.5
pyarrow==1.0.0
pytesseract==0.3.5
pytest-cov==2.10.1
pytest>=5.2
ray==0.8.7
tqdm>=4.41.0

================================================
FILE: .flake8
================================================
[flake8]

exclude =
  __init__.py,
  __pycache__/*,
  env/*,
  venv/*,
  .env/*,
  .venv/*,

================================================
FILE: pdf2dataset/__init__.py
================================================
from .utils import *
from .extraction import Extraction
from .extraction_memory import ExtractionFromMemory
from .extract_task import ExtractTask, feature
from .pdf_extract_task import PdfExtractTask

__all__ = ['Extraction', 'ExtractionFromMemory', 'ExtractTask',\
           'feature', 'extract'] + HELPERS

================================================
FILE: pdf2dataset/__main__.py
================================================
import argparse

from . import Extraction
from .pdf_extract_task import PdfExtractTask

# TODO: use click
def main():
    parser = argparse.ArgumentParser(
        description='Extract text from all PDF files in a directory'
    )
    parser.add_argument(
        'input_dir',
        type=str,
        help='The folder to lookup for PDF files recursively'
    )
    parser.add_argument(
        'out_file',
        type=str,
        default='df.parquet.gzip',
        help='File to save the resultant dataframe'
    )

    available_featues = ', '.join(PdfExtractTask.list_features())
    parser.add_argument(
        '--features',
        type=str,
        default='all',
        help=(
            'Specify a comma separated list with the features you want to'
            " extract. 'path' and 'page' will always be added."
            f' Available features to add: {available_featues}'
            " Examples: '--features=text,image' or '--features=all'"
        )
    )

    parser.add_argument(
        '--saving-interval',
        type=int,
        default=5000,
        help=('Results will be persisted to results folder'
              ' every saving interval of pages')
    )
    parser.add_argument(
        '--ocr-lang',
        type=str,
        default='por',
        help='Tesseract language'
    )
    parser.add_argument(
        '--ocr',
        type=bool,
        default=False,
        help="'pytesseract' if true, else 'pdftotext'. default: false"
    )
    parser.add_argument(
        '--chunksize',
        type=int,
        help="Chunksize to use while processing pages, otherwise is calculated"
    )
    parser.add_argument(
        '--image-size',
        type=str,
        default=None,
        help=(
            'If adding image feature, image will be resized to this size.'
            " Provide two integers separated by 'x'."
            ' Example: --image-size 1000x1414'
        )
    )
    parser.add_argument(
        '--ocr-image-size',
        type=int,
        default=None,
        help=(
            'The height of the image OCR will be applied.'
            ' Width will be adjusted to keep the ratio.'
        )
    )
    parser.add_argument(
        '--image-format',
        type=str,
        default='jpeg',
        help=(
            'Format of the image generated from the PDF pages'
        )
    )

    # Ray
    parser.add_argument(
        '--num-cpus',
        type=int,
        help='Number of cpus to use'
    )
    parser.add_argument(
        '--address',
        type=str,
        help='Ray address to connect'
    )
    parser.add_argument(
        '--dashboard-host',
        type=str,
        default='*',
        help='Which IP ray webui will try to listen on'
    )
    parser.add_argument(
        '--redis-password',
        type=str,
        default='5241590000000000',  # Ray default
        help='Redis password to use to connect with ray'
    )

    args = parser.parse_args()

    extraction = Extraction(**vars(args))
    extraction.apply()

    print(f"Results saved to '{extraction.out_file}'!")

if __name__ == '__main__':
    main()

================================================
FILE: pdf2dataset/extract_task.py
================================================
import traceback
from abc import ABC
from copy import deepcopy
from itertools import chain
from inspect import getmembers, isroutine
from functools import wraps

import pyarrow as pa

def feature(pyarrow_type=None, is_helper=False, exceptions=None, **type_args):
    exceptions = exceptions or tuple()
    exceptions = tuple(exceptions)

    if not (pyarrow_type or is_helper):
        raise ValueError("If feature is not helper, must set 'pyarrow_type'")

    def decorator(feature_method):
        feature_method.pyarrow_type = None
        feature_method.is_feature = True
        feature_method.is_helper = is_helper

        if pyarrow_type is not None:
            type_ = getattr(pa, pyarrow_type)(**type_args)

            if isinstance(type_, pa.DataType):
                feature_method.pyarrow_type = type_
            else:
                raise ValueError(f'Invalid PyArrow type {pyarrow_type}!')

        @wraps(feature_method)
        def inner(*args, **kwargs):
            result, error = None, None

            try:
                result = feature_method(*args, **kwargs)
            except exceptions:
                error = traceback.format_exc()

            return result, error
        return inner

    return decorator

# TODO: Eventually, I'll make this a new lib
class ExtractTask(ABC):

    fixed_featues = ('path',)
    _feature_prefix = 'get_'  # Optional

    def __init__(self, path, file_bin=None, sel_features='all'):
        self.path = path
        self.file_bin = file_bin
        self.sel_features = self._parse_sel_features(sel_features)

        self._features = {}
        self._errors = {}

        self._init_all_features()

    def __init_subclass__(cls, **kwargs):
        # Memoization
        cls._helper_list = None
        cls._features_list = {}

    @classmethod
    def list_helper_features(cls):
        if cls._helper_list is not None:
            return cls._helper_list.copy()

        prefix = cls._feature_prefix

        def is_helper(name, method):
            return (getattr(method, 'is_helper', False)
                    and name.startswith(prefix))

        class_routines = getmembers(cls, predicate=isroutine)

        cls._helper_list = [n[len(prefix):]\
                            for n, m in class_routines if is_helper(n, m)]

        return cls._helper_list

    @classmethod
    def list_features(cls, *, exclude_fixed=False):
        is_calculated = cls._features_list.get(exclude_fixed)

        if is_calculated:
            return cls._features_list[exclude_fixed].copy()

        def include(name, method):
            helper = [cls._get_feature_methodname(f)\
                      for f in cls.list_helper_features()]

            is_feature = getattr(method, 'is_feature', False)
            feat_name = name[len(cls._feature_prefix):]

            return (is_feature
                    and name not in helper
                    and name.startswith(cls._feature_prefix)
                    and not (feat_name in cls.fixed_featues and exclude_fixed))

        class_routines = getmembers(cls, predicate=isroutine)

        features_list = [n[len(cls._feature_prefix):]\
                         for n, m in class_routines if include(n, m)]

        cls._features_list[exclude_fixed] = features_list
        return features_list

    @classmethod
    def get_schema(cls, features=()):
        def get_type(feature_name):
            method_name = cls._get_feature_methodname(feature_name)
            method = getattr(cls, method_name)

            if method.is_helper:
                return None

            return method.pyarrow_type

        class_features = cls.list_features()
        names = (name for name in features if name in class_features)

        features_types = ((name, get_type(name)) for name in names)

        features_types = [(f, t) for f, t in features_types if t is not None]
        features_types.append(('error', pa.string()))

        return pa.schema(features_types)

    @classmethod
    def _get_feature_methodname(cls, feature_name):
        method_name = cls._feature_prefix + feature_name

        if not hasattr(cls, method_name):
            raise RuntimeError(f"Method '{method_name}' not found!")

        return method_name

    def list_instance_features(self):
        return list(chain(self.fixed_featues, self.sel_features, ['error']))

    def load_bin(self, enforce=False):
        '''
        Loads the file binary

        Should not be called inside its class, as the node running
        this task might not have access to the file in his filesystem
        '''
        if enforce or not self.file_bin:
            self.file_bin = self.path.read_bytes()

    def copy(self):
        return deepcopy(self)

    def get_feature(self, name):
        extract_method_name = self._get_feature_methodname(name)
        extract_method = getattr(self, extract_method_name)

        if self._features[name] is None and self._errors[name] is None:
            self._features[name], self._errors[name] = extract_method()

        return self._features[name], self._errors[name]

    def process(self):
        if not self.file_bin:
            raise RuntimeError(
                "'file_bin' can't be empty for processing the task!"
            )

        return self._gen_result()

    def _gen_result(self):
        expected_features = chain(self.fixed_featues, self.sel_features)

        result = {name: self.get_feature(name)[0]
                  for name in expected_features}
        result['error'] = self._gen_errors_string()

        return result

    def _gen_errors_string(self):
        features_errors = (f'{f}:\n{e}' for f, e in self._errors.items() if e)
        all_errors = '\n\n\n'.join(features_errors)

        return all_errors or None

    def _init_all_features(self):
        helper = self.list_helper_features()
        features = chain(self.fixed_featues, helper, self.sel_features)

        self._features = {f: None for f in features}
        self._errors = deepcopy(self._features)

    def _parse_sel_features(self, sel_features):
        possible_features = self.list_features()

        if sel_features == '':
            sel_features = []

        elif sel_features == 'all':
            sel_features = possible_features

        elif isinstance(sel_features, list):
            ...

        else:
            sel_features = sel_features.split(',')

        failed = (f not in possible_features for f in sel_features)
        if any(failed):
            sel_features = ','.join(sel_features)
            possible_features = ','.join(possible_features)

            raise ValueError(
                f"Invalid feature list: '{sel_features}'"
                f"\nPossible features are: '{possible_features}'"
            )

        return sel_features

================================================
FILE: pdf2dataset/extraction.py
================================================
import io
import itertools as it
import logging
import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Pool, Process, Queue
from pathlib import Path
from more_itertools import ichunked

import ray
import pdftotext
from tqdm import tqdm
from pytesseract import get_tesseract_version

from .pdf_extract_task import PdfExtractTask
from .results import Results

# TODO: Eventually, I'll reduce this file size!!

# TODO: Add typing?
# TODO: Set up a logger to the class
# TODO: Substitute most (all?) prints for logs

class Extraction:

    def __init__(
        self, input_dir=None, out_file=None, *,
        files_list=None, task_class=PdfExtractTask,

        # Config params
        small=False, check_input=True, chunksize=None,
        saving_interval=5000, max_files_memory=3000, files_pattern='*.pdf',

        # Task_params
        ocr=False, ocr_image_size=None, ocr_lang='por', features='all',
        image_format='jpeg', image_size=None,

        **ray_params
    ):
        self.input_dir = Path(input_dir).resolve() if input_dir else None
        self.files_list = [Path(f) for f in files_list] if files_list else None

        self.out_file = Path(out_file).resolve() if out_file else None

        if check_input:
            self._check_input()

        if not small:
            self._check_outfile()

        if ocr:
            # Will raise exception if tesseract was not found
            get_tesseract_version()

        self.num_cpus = ray_params.get('num_cpus') or os.cpu_count()
        self.ray_params = ray_params
        self.chunksize = chunksize
        self.small = small
        self.max_files_memory = max_files_memory
        self.files_pattern = files_pattern

        self.num_skipped = None

        self.task_class = task_class
        self.task_params = {
            'sel_features': features,
            'ocr': ocr,
            'ocr_lang': ocr_lang,
            'ocr_image_size': ocr_image_size,
            'image_format': image_format,
            'image_size': image_size,
        }

        columns = self.list_columns()
        schema = self.task_class.get_schema(columns)

        max_results_size = saving_interval if not small else None
        self.results = Results(
            self.input_dir, self.out_file, schema, max_size=max_results_size
        )

        self.results_queue = Queue(max_files_memory)

    def _check_input(self):
        if not any([self.input_dir, self.files_list]):
            raise RuntimeError(
                "Any of 'input_dir' or 'self.files_list' must be provided"
            )

        if not self.files_list:
            self._check_inputdir()

    def _check_inputdir(self):
        if (not self.input_dir
                or not (self.input_dir.exists() and self.input_dir.is_dir())):

            raise ValueError(f"Invalid input_dir: '{self.input_dir}',"
                             " it must exists and be a directory")

    def _check_outfile(self):
        if not self.out_file:
            raise RuntimeError("If not using 'small' arg,"
                               " 'out_file' is mandatory")

    @property
    def files(self):
        if self.files_list:
            return self.files_list

        self.files_list = self.list_files()
        return self.files_list

    def list_columns(self):
        aux_task = self.task_class(1, 1, **self.task_params)

        columns = aux_task.sel_features

        begin = list(aux_task.fixed_featues)
        columns = begin + sorted(c for c in columns if c not in begin)

        columns.append('error')  # Always last

        return columns

    def list_files(self):
        pdf_files = self.input_dir.rglob(self.files_pattern)

        # Here feedback is better than keeping use of the generator
        return list(tqdm(pdf_files, desc='Looking for files', unit='files'))

    def gen_tasks(self):
        '''
        Returns tasks to be processed.
        For faulty files, only the page -1 will be available
        '''
        if self.num_cpus == 1:
            return self._gen_tasks_sequential()

        # 10 because this is a fast operation
        chunksize = int(max(1, (len(self.files)/self.num_cpus)//10))

        tasks = []
        with Pool(self.num_cpus) as pool, \
                tqdm(desc='Counting pages', unit='pages') as pbar:

            results = pool.imap(
                self.get_pages_range, self.files, chunksize=chunksize
            )

            for path, range_pages in zip(self.files, results):
                new_tasks = [\
                    self.task_class(path, p, **self.task_params)\
                    for p in range_pages\
                ]
                tasks += new_tasks
                pbar.update(len(range_pages))

        return tasks

    def _gen_tasks_sequential(self):
        tasks = []

        with tqdm(desc='Counting pages', unit='pages') as pbar:
            results = map(self.get_pages_range, self.files)

            for path, range_pages in zip(self.files, results):
                new_tasks = [\
                    self.task_class(path, p, **self.task_params)\
                    for p in range_pages\
                ]
                tasks += new_tasks
                pbar.update(len(range_pages))

        return tasks

    @staticmethod
    def get_pages_range(file_path, file_bin=None):
        # Using pdftotext to get num_pages because it's the best way I know
        # pdftotext extracts lazy, so this won't process the text

        try:
            if not file_bin:
                with file_path.open('rb') as f:
                    num_pages = len(pdftotext.PDF(f))
            else:
                with io.BytesIO(file_bin) as f:
                    num_pages = len(pdftotext.PDF(f))

            pages = range(1, num_pages+1)
        except pdftotext.Error:
            pages = [-1]

        return pages

    def _get_processing_bar(self, num_tasks, iterable=None):
        num_skipped = self.num_skipped or 0

        return tqdm(
            iterable, total=num_tasks+num_skipped, initial=num_skipped,
            desc='Processing pages', unit='pages', dynamic_ncols=True
        )

    @staticmethod
    def copy_and_load_task(task):
        task = task.copy()  # Copy to have control over memory references
        task.load_bin()

        return task

    @staticmethod
    @ray.remote
    def _process_chunk_ray(chunk):
        return [t.process() for t in chunk]

    @staticmethod
    def _submit_chunk_ray(chunk):
        with ThreadPoolExecutor() as executor:
            chunk = list(executor.map(Extraction.copy_and_load_task, chunk))

        return Extraction._process_chunk_ray.remote(chunk)

    def _ray_process_aux(self, tasks, results_queue):
        chunks = ichunked(tasks, int(self.chunksize))
        num_initial = int(ray.available_resources()['CPU'])

        with self._get_processing_bar(len(tasks)) as progress_bar:
            futures = [self._submit_chunk_ray(c)\
                       for c in it.islice(chunks, num_initial)]

            while futures:
                (finished, *_), rest = ray.wait(futures, num_returns=1)

                result = ray.get(finished)
                results_queue.put(result)

                progress_bar.update(len(result))

                try:
                    chunk = next(chunks)
                except StopIteration:
                    ...
                else:
                    rest.append(self._submit_chunk_ray(chunk))

                futures = rest

        results_queue.put(None)

    def _ray_process(self, *args, **kwargs):
        ray.init(**self.ray_params)

        try:
            self._ray_process_aux(*args, **kwargs)
        finally:
            ray.shutdown()

    def _apply_to_big(self, tasks):
        'Apply the extraction to a big volume of data'

        print('\n=== SUMMARY ===',
              f'PDFs directory: {self.input_dir}',
              f'Results file: {self.out_file}',
              f'Using {self.num_cpus} CPU(s)',
              f'Chunksize: {self.chunksize}',
              sep='\n', end='\n\n')

        Process(
            target=self._ray_process,
            args=(tasks, self.results_queue)
        ).start()

        while True:
            result_chunk = self.results_queue.get()

            if isinstance(result_chunk, Exception):
                raise result_chunk

            if result_chunk is None:
                break

            self.results.append(result_chunk)

        if self.results:
            self.results.write_and_clear()

    @staticmethod
    def _process_task(task):
        return task.process()

    def _apply_to_small(self, tasks):
        ''''Apply the extraction to a small volume of data
        More direct approach than 'big', but with these differences:
            - Not saving progress
            - Distributed processing not supported
            - Don't write dataframe to disk
            - Returns the resultant dataframe
        '''

        if self.num_cpus == 1:
            return self._apply_sequential(tasks)

        num_tasks = len(tasks)
        tasks = (self.copy_and_load_task(t) for t in tasks)

        with Pool(self.num_cpus) as pool:
            processing_tasks = pool.imap_unordered(
                self._process_task, tasks, self.chunksize
            )
            results = self._get_processing_bar(num_tasks, processing_tasks)

            self.results.append(results)

        return self.results.get()

    def _apply_sequential(self, tasks):
        num_tasks = len(tasks)
        tasks = (self.copy_and_load_task(t) for t in tasks)

        processing_tasks = map(self._process_task, tasks)

        results = self._get_processing_bar(num_tasks, processing_tasks)
        self.results.append(results)

        return self.results.get()

    def filter_processed_tasks(self, tasks):
        is_processed = self.results.is_tasks_processed(tasks)
        tasks = [t for t, is_ in zip(tasks, is_processed) if not is_]

        return tasks

    def _process_tasks(self, tasks):
        if self.chunksize is None:
            chunk_by_cpu = (len(tasks)/self.num_cpus) / 100
            max_chunksize = self.max_files_memory // self.num_cpus
            self.chunksize = int(max(1, min(chunk_by_cpu, max_chunksize)))

        if self.small:
            return self._apply_to_small(tasks)

        self._apply_to_big(tasks)

        return self.out_file

    def apply(self):
        tasks = self.gen_tasks()

        num_total_tasks = len(tasks)
        tasks = self.filter_processed_tasks(tasks)
        self.num_skipped = num_total_tasks - len(tasks)

        if self.num_skipped:
            logging.warning(
                "'%s' have already %d processed pages, skipping these...",
                self.out_file, self.num_skipped
            )

        return self._process_tasks(tasks)

================================================
FILE: pdf2dataset/extraction_memory.py
================================================
from pathlib import Path

from .extraction import Extraction

class ExtractionFromMemory(Extraction):

    def __init__(self, tasks, *args, **kwargs):
        self.tasks = tasks

        kwargs['check_input'] = False

        super().__init__(None, *args, **kwargs)

    def gen_tasks(self):
        # Just to make more semantic
        return self._gen_extrationtasks(self.tasks)

    def _gen_extrationtasks(self, tasks):
        ''' Generate extraction task from simplified tasks form.

        Assumes is not a big volume, otherwise should save files to
        a directory and use 'Extraction'. So, not going with
        multiprocessing here.
        '''

        def uniform(task):
            page = None

            if len(task) == 2:
                file_, file_bin = task
            elif len(task) == 3:
                file_, file_bin, page = task
            else:
                raise RuntimeError(
                    'Wrong task format, it must be'
                    ' (file_name, file_bin)'
                    ' or (file_name, file_bin, page_number)'
                )

            if not str(file_).endswith('.pdf'):
                raise RuntimeError(
                    f"Document '{file_}' name must ends with '.pdf'"
                )

            range_pages = self.get_pages_range(file_, file_bin=file_bin)

            # -1 specifically because of the flag used by _get_pages_range
            if page in range_pages and not page == -1:
                range_pages = [page]

            elif page is not None:
                raise RuntimeError(
                    f"Page {page} doesn't exist in file {file_}!"
                )

            return Path(file_).resolve(), file_bin, range_pages

        tasks = [uniform(t) for t in tasks]

        new_tasks = []
        for file_, file_bin, range_pages in tasks:
            new_tasks += [\
                self.task_class(file_, p, file_bin, **self.task_params)\
                for p in range_pages\
            ]

        return new_tasks

    def apply(self):
        # Coverts simplified notation tasks to extraction tasks
        tasks = self.gen_tasks()

        return self._process_tasks(tasks)

================================================
FILE: pdf2dataset/pdf_extract_task.py
================================================
import io
import os

import numpy as np
import pytesseract
import cv2
import pdftotext
from pdf2image import convert_from_bytes
from pdf2image.exceptions import PDFPageCountError, PDFSyntaxError
from PIL import Image as PilImage
from PIL.Image import DecompressionBombError

from .extract_task import ExtractTask, feature

class Image:

    def __init__(self, image, image_format=None):
        self.pil_image = image
        self.image_format = image_format or self.pil_image.format

    @classmethod
    def from_bytes(cls, image_bytes):
        image = PilImage.open(io.BytesIO(image_bytes))
        return cls(image)

    def resize(self, size):
        pil_image = self.pil_image.resize(size)
        return type(self)(pil_image, self.image_format)

    def preprocess(self):
        image = np.asarray(self.pil_image.convert('L'))
        image = cv2.adaptiveThreshold(
            image, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            97, 50
        )

        image = cv2.erode(
            image,
            cv2.getStructuringElement(cv2.MORPH_CROSS, (2, 2)),
            iterations=1
        )

        pil_image = PilImage.fromarray(image)
        return type(self)(pil_image)

    @staticmethod
    def parse_size(image_size_str):
        if image_size_str is None:
            return None

        image_size_str = image_size_str.lower()

        try:
            width, height = map(int, image_size_str.split('x'))
        except ValueError as e:
            raise ValueError(
                f'Invalid image size parameter: {image_size_str}'
            ) from e

        return width, height

    def ocr(self, lang='por'):
        # So pytesseract uses only one core per worker
        os.environ['OMP_THREAD_LIMIT'] = '1'
        return pytesseract.image_to_string(self.pil_image, lang=lang)

    def to_bytes(self):
        image_stream = io.BytesIO()

        with io.BytesIO() as image_stream:
            self.pil_image.save(image_stream, self.image_format)
            image_bytes = image_stream.getvalue()

        return image_bytes

class PdfExtractTask(ExtractTask):

    class OcrError(Exception):
        ...

    fixed_featues = ('path', 'page')

    def __init__(self, path, page, *args,
                 ocr=False, ocr_image_size=None, ocr_lang='por',
                 image_format='jpeg', image_size=None, **kwargs):

        self.page = page
        self.ocr = ocr
        self.ocr_lang = ocr_lang
        self.ocr_image_size = ocr_image_size
        self.image_format = image_format
        self.image_size = Image.parse_size(image_size)

        super().__init__(path, *args, **kwargs)

    def __repr__(self):
        return f'PdfExtractTask({self.path}, {self.page})'

    def _extract_text_ocr(self):
        image_bytes, _ = self.get_feature('image_original')

        if not image_bytes:
            raise self.OcrError("Wasn't possible to get page image!")

        image = Image.from_bytes(image_bytes)
        preprocessed = image.preprocess()

        return preprocessed.ocr()

    def _extract_text_native(self):
        with io.BytesIO(self.file_bin) as f:
            pages = pdftotext.PDF(f)
            text = pages[self.page-1]

        return text

    @feature(
        'binary', is_helper=True,
        exceptions=(PDFPageCountError, PDFSyntaxError, DecompressionBombError)
    )
    def get_image_original(self):
        images = convert_from_bytes(
            self.file_bin, first_page=self.page,
            single_file=True, fmt=self.image_format,
            size=(None, self.ocr_image_size)
        )

        image = Image(images[0])

        return image.to_bytes()

    @feature('int16')
    def get_page(self):
        return self.page

    @feature('string')
    def get_path(self):
        return str(self.path)

    @feature('binary')
    def get_image(self):
        image_bytes, _ = self.get_feature('image_original')

        if not image_bytes:
            return None

        if self.image_size:
            image = Image.from_bytes(image_bytes)
            size = self.image_size
            image_bytes = image.resize(size).to_bytes()

        return image_bytes

    @feature('string', exceptions=[pdftotext.Error, OcrError])
    def get_text(self):
        if self.ocr:
            return self._extract_text_ocr()

        return self._extract_text_native()

================================================
FILE: pdf2dataset/results.py
================================================
from pathlib import Path
import pyarrow as pa

import pandas as pd
import dask.dataframe as dd

class Results:

    def __init__(self, input_dir, out_file, schema, *, max_size=None):
        self.input_dir = input_dir
        self.out_file = out_file
        self.schema = schema
        self.max_size = max_size

        self.results_dicts = []

    def __len__(self):
        return len(self.results_dicts)

    def _path_to_relative(self, file_path):
        if file_path is None:
            return None

        file_path = Path(file_path).resolve()
        input_dir = self.input_dir or Path().resolve()

        try:
            file_path = file_path.relative_to(input_dir)
        except ValueError:
            ...

        return str(file_path)

    def _results_as_df(self):
        if not self.results_dicts:
            table = pa.Table.from_arrays(
                [[]]*len(self.schema),
                schema=self.schema
            )

            return table.to_pandas()

        df = pd.DataFrame(self.results_dicts)
        df['path'] = df['path'].apply(self._path_to_relative)

        return df

    def write_and_clear(self):
        if self.results_dicts == []:
            raise RuntimeError('Trying to write empty results!')

        if self.out_file is None:
            raise ValueError("For writting 'out_file' must be provided")

        df = self._results_as_df()
        ddf = dd.from_pandas(df, npartitions=1)

        exists = self.out_file.exists()

        # Dask has some optimizations over pure pyarrow,
        #   like handling _metadata
        ddf.to_parquet(
            self.out_file, compression='gzip',
            ignore_divisions=True, write_index=False,
            schema=self.schema, append=exists, engine='pyarrow'
        )
        self.results_dicts = []

    def append(self, results_dicts):
        results_dicts = list(results_dicts)

        self.results_dicts.extend(results_dicts)

        if (self.max_size is not None and
                len(self.results_dicts) >= self.max_size):
            self.write_and_clear()

    def get(self):
        if self.max_size:
            raise RuntimeError(
                "Can't get() when using 'max_size' option,"
                " results would be incomplete"
            )

        return self._results_as_df()

    def is_tasks_processed(self, tasks):
        def gen_is_processed(df):
            num_checked = 0
            all_tasks = set(tuple(task) for task in df.itertuples(index=False))

            for task in tasks:
                if num_checked == len(tasks):
                    break  # All processed tasks were counted

                path = self._path_to_relative(task.path)

                is_processed = (path, task.page) in all_tasks
                num_checked += int(is_processed)

                yield is_processed

        try:
            df = pd.read_parquet(
                self.out_file,
                engine='pyarrow',
                columns=['path', 'page']
            )
        except (FileNotFoundError, ValueError):
            return (False for _ in tasks)

        return gen_is_processed(df)

================================================
FILE: pdf2dataset/utils.py
================================================
import sys
import inspect

from .extraction import Extraction
from .extraction_memory import ExtractionFromMemory
from .pdf_extract_task import PdfExtractTask, Image

def image_from_bytes(image_bytes):
    image = Image.from_bytes(image_bytes)
    return image.pil_image

def image_to_bytes(pil_image, image_format='jpeg'):
    image = Image(pil_image, image_format)
    return image.to_bytes()

def _df_to_list(df, extraction):
    fixed_featues = list(extraction.task_class.fixed_featues)

    df.set_index(fixed_featues, inplace=True)
    df.sort_index(inplace=True)
    df.pop('error')  # ignoring errors when returning list

    df = df[sorted(df.columns)]
    df['features'] = df.apply(lambda row: row.to_list(), axis='columns')

    groups = df.reset_index().groupby('path')['features']

    return groups.agg(list).to_list()

def extract(*args, return_list=False, **kwargs):
    def is_tasks(input_):
        return (
            kwargs.get('tasks')
            or (isinstance(input_, (list, tuple))
                and input_ and isinstance(input_[0], (list, tuple)))
        )

    args = list(args)
    input_ = args.pop(0) if args else None

    kwargs['small'] = True if return_list else kwargs.get('small', False)

    if inspect.isgenerator(input_):
        raise ValueError('Generator as input is not currently supported!')

    if is_tasks(input_):
        tasks = kwargs.pop('tasks', input_)
        extraction = ExtractionFromMemory(tasks, *args, **kwargs)
    else:
        if isinstance(input_, (list, tuple)):
            kwargs['files_list'] = input_
            input_ = ''

        extraction = Extraction(input_, *args, **kwargs)

    df = extraction.apply()  # df is None if small equals False

    if return_list:
        return _df_to_list(df, extraction)

    return df

def gen_helpers(task_class=PdfExtractTask):
    def helpers_factory(feature):
        def helper(*args, **kwargs):
            kwargs['features'] = (feature if feature not in
                                  task_class.fixed_featues else '')

            return extract(*args, **kwargs)

        return helper

    def gen_helper(feature):
        method_name = f'extract_{feature}'
        setattr(sys.modules[__name__], method_name, helpers_factory(feature))

        return method_name

    features = task_class.list_features()
    return [gen_helper(feature) for feature in features]

HELPERS = gen_helpers()

================================================
FILE: tests/__init__.py
================================================
import pytest

pytest.register_assert_rewrite('tests.testing_dataframe')

================================================
FILE: tests/conftest.py
================================================
import pytest
from pathlib import Path
import pandas as pd

SAMPLES_DIR = Path('tests/samples')
SAMPLE_IMAGE = SAMPLES_DIR / 'single_page1_1.jpeg'
PARQUET_ENGINE = 'pyarrow'

@pytest.fixture
def complete_df():

    def read_image(path, page):
        if page == -1:
            return None

        path = Path(path).with_suffix('')
        image_name = f'{path}_{page}.jpeg'
        image_path = Path(SAMPLES_DIR) / image_name

        with open(image_path, 'rb') as f:
            image_bin = f.read()

        return image_bin

    rows = [\
        ['path', 'page', 'text', 'error_bool'],\
\
        ['multi_page1.pdf', 1, 'First page', False],\
        ['multi_page1.pdf', 2, 'Second page', False],\
        ['multi_page1.pdf', 3, 'Third page', False],\
        ['sub1/copy_multi_page1.pdf', 1, 'First page', False],\
        ['sub1/copy_multi_page1.pdf', 2, 'Second page', False],\
        ['sub1/copy_multi_page1.pdf', 3, 'Third page', False],\
        ['single_page1.pdf', 1, 'My beautiful sample!', False],\
        ['sub2/copy_single_page1.pdf', 1, 'My beautiful sample!', False],\
        ['invalid1.pdf', -1, None, True]\
    ]

    names = rows.pop(0)
    expected_dict = {n: r for n, r in zip(names, zip(*rows))}

    df = pd.DataFrame(expected_dict)
    df['image'] = df.apply(lambda row: read_image(row.path, row.page), axis=1)

    return df

================================================
FILE: tests/test_extract_task.py
================================================
import pytest
from pathlib import Path

import pyarrow as pa
import numpy as np
from PIL import Image
from pdf2dataset import (
    PdfExtractTask,
    extract,
    feature,
    image_to_bytes,
    image_from_bytes,
)

from .conftest import SAMPLES_DIR, SAMPLE_IMAGE

class MyCustomTask(PdfExtractTask):

    @feature('bool_')
    def get_is_page_even(self):
        return self.page % 2 == 0

    @feature(is_helper=True)
    def get_doc_first_bytes(self):
        return self.file_bin[:10]

    @feature('list_', value_type=pa.string())
    def get_list(self):
        return ['E0', 'E1', 'My super string!']

    @feature('string', exceptions=[ValueError])
    def get_wrong(self):
        raise ValueError("There was a problem!")

@pytest.fixture
def image():
    return Image.open(SAMPLE_IMAGE)

@pytest.fixture
def image_bytes():
    with open(SAMPLE_IMAGE, 'rb') as f:
        bytes_ = f.read()

    return bytes_

def test_imagefrombytes(image, image_bytes):

    assert image_from_bytes(image_bytes) == image

def test_imagetobytes(image, image_bytes):
    # png because jpeg change pixel values
    calculated = image_from_bytes(image_to_bytes(image, 'png'))

    assert (np.array(calculated) == np.array(image)).all()

def test_list_features():
    inherited_features = PdfExtractTask.list_features()
    custom_features = MyCustomTask.list_features()

    # 3 because I've defined this number of (not helpers) custom features
    expected_num_features = len(inherited_features) + 3
    assert expected_num_features == len(custom_features)

    assert set(inherited_features) < set(custom_features)

    assert set(['is_page_even', 'wrong', 'list']) < set(custom_features)

def test_list_helper_features():
    inherited_features = PdfExtractTask.list_helper_features()
    custom_features = MyCustomTask.list_helper_features()

    # 1 because I've defined one helpers custom feature
    expected_num_features = len(inherited_features) + 1
    assert expected_num_features == len(custom_features)

    assert set(inherited_features) < set(custom_features)

    assert set(['doc_first_bytes']) < set(custom_features)

def test_saving_to_disk(tmp_path):
    out_file = tmp_path / 'my_df.parquet.gzip'
    extract(SAMPLES_DIR, out_file, task_class=MyCustomTask)

    assert Path(out_file).exists()

def test_columns_present():
    df = extract('tests/samples', small=True, task_class=MyCustomTask)
    assert set(MyCustomTask.list_features()) < set(df.columns)

def test_error_recorded():
    df = extract('tests/samples', small=True, task_class=MyCustomTask)
    error_feature, error_msg = 'wrong', 'There was a problem'

    assert error_msg in df.iloc[0].error
    assert f'{error_feature}:' in df.iloc[0].error

================================================
FILE: tests/test_extraction.py
================================================
from io import BytesIO
from pathlib import Path
from hashlib import md5

import pytest
import pandas as pd
from PIL import Image

from pdf2dataset import (
    ExtractionFromMemory,
    PdfExtractTask,
    extract,
    extract_text
)

from .testing_dataframe import check_and_compare
from .conftest import SAMPLES_DIR, PARQUET_ENGINE

class TestExtractionCore:

    @pytest.mark.parametrize('is_ocr', (
        True,
        False,
    ))
    def test_extraction_big(self, tmp_path, is_ocr, complete_df):
        result_path = tmp_path / 'result.parquet.gzip'

        extract(SAMPLES_DIR, result_path,
                ocr_lang='eng', ocr=is_ocr, features='all')

        df = pd.read_parquet(result_path, engine=PARQUET_ENGINE)

        if is_ocr:
            df['text'] = df['text'].str.strip()

        check_and_compare(df, complete_df, is_ocr=is_ocr)

    def test_append_result(self, tmp_path, complete_df):
        result_path = tmp_path / 'result.parquet.gzip'

        extract(SAMPLES_DIR, result_path, saving_interval=1, features='all')

        # Small 'chunk_df_size' to append to result multiple times
        df = pd.read_parquet(result_path, engine=PARQUET_ENGINE)

        check_and_compare(df, complete_df)

    def test_passing_paths_list(self, tmp_path, complete_df):
        result_path = tmp_path / 'result.parquet.gzip'
        files_list = Path(SAMPLES_DIR).rglob('*.pdf')

        # Test the support for paths as strings
        files_list = [str(f) for f in files_list]

        df = extract(files_list, result_path, small=True)

        # Paths will be relative to pwd, so adapting complete_df
        complete_df['path'] = complete_df['path'].apply(
            lambda p: str(SAMPLES_DIR / p)
        )
        check_and_compare(df, complete_df)

    def test_filter_processed(self, tmp_path):
        with open(SAMPLES_DIR / 'single_page1.pdf', 'rb') as f:
            single = f.read()

        with open(SAMPLES_DIR / 'multi_page1.pdf', 'rb') as f:
            multi = f.read()

        with open(SAMPLES_DIR / 'invalid1.pdf', 'rb') as f:
            invalid = f.read()

        total_tasks = [\
            ('single1.pdf', single),\
            ('multi1.pdf', multi, 2),\
            ('hey/multi2.pdf', multi, 1),\
            ('multi1.pdf', multi, 1),\
            ('my_dir/single3.pdf', single),\
            ('/opt/invalid.pdf', invalid),\
            ('multi1.pdf', multi, 3),\
            ('single2.pdf', single),\
            ('/tmp/single3.pdf', single),\
            ('/tmp/invalid.pdf', invalid),\
        ]

        result_path = tmp_path / 'result.parquet.gzip'

        for counter, task in enumerate(total_tasks):

            extraction = ExtractionFromMemory(
                total_tasks,
                out_file=result_path,
                features='text'
            )

            tasks = extraction.gen_tasks()
            tasks = extraction.filter_processed_tasks(tasks)

            assert len(tasks) == len(total_tasks) - counter

            extract([task], result_path, features='text')

class TestExtractionSmall:
    @pytest.mark.parametrize('is_ocr', (
        True,
        False,
    ))
    def test_extraction_small(self, is_ocr, complete_df):
        df = extract(SAMPLES_DIR, small=True, ocr_lang='eng', ocr=is_ocr)

        if is_ocr:
            df['text'] = df['text'].str.strip()

        check_and_compare(df, complete_df, is_ocr=is_ocr)

    def test_return_list(self):
        def sort(doc):
            try:
                first_page_idx, text_idx = 0, 1

                return len(doc[first_page_idx][text_idx])
            except TypeError:
                return -1  # For None values

        def hash_images(doc):
            for page_idx, page in enumerate(doc):
                image, text = page
                image = md5(image).hexdigest() if image is not None else None

                doc[page_idx] = [image, text]

            return doc

        list_ = extract(SAMPLES_DIR, return_list=True)
        list_ = [hash_images(doc) for doc in list_]

        # Expected structure:
        #   expected: list[doc],
        #   doc: list[page],
        #   page: list[feature]
        #
        # list[feature] is sorted by feature name (not value!)
        expected = [\
            # invalid1.pdf\
            [[None, None]],\
\
            # single_page.pdf\
            [['975f5049aac2d0b0e85e0083657182fd', 'My beautiful sample!']],\
\
            # sub2/copy_single_page.pdf\
            [['975f5049aac2d0b0e85e0083657182fd', 'My beautiful sample!']],\
\
            # multi_page.pdf\
            [['5f005131323536c524e0fffa7ab42d0f', 'First page'],\
                ['fce06f79de9ca575212152873cb161ea', 'Second page'],\
                ['6dacd3629627df0d99ebc5da97b310b2', 'Third page']],\
\
            # sub1/copy_multi_page.pdf\
            [['5f005131323536c524e0fffa7ab42d0f', 'First page'],\
                ['fce06f79de9ca575212152873cb161ea', 'Second page'],\
                ['6dacd3629627df0d99ebc5da97b310b2', 'Third page']],\
        ]

        assert sorted(list_, key=sort) == sorted(expected, key=sort)

class TestParams:
    def test_features_as_list(self, complete_df):
        df = extract(SAMPLES_DIR, small=True, features=['text', 'image'])
        check_and_compare(df, complete_df)

    @pytest.mark.parametrize('excluded', [\
        'text',\
        'image',\
    ])
    def test_exclude_feature(self, excluded, complete_df):
        features = PdfExtractTask.list_features()
        features.remove(excluded)

        df = extract(SAMPLES_DIR, small=True, features=features)

        columns = list(complete_df.columns)
        columns.remove(excluded)

        check_and_compare(df, complete_df[columns])

    def test_empty_feature(self, complete_df):
        df = extract(SAMPLES_DIR, small=True, features='')

        columns = list(PdfExtractTask.fixed_featues) + ['error_bool']
        check_and_compare(df, complete_df[columns])

    @pytest.mark.parametrize('size', (
        ('10x10'),
        ('10X10'),
        ('10x100'),
    ))
    def test_image_resize(self, size):
        df = extract(SAMPLES_DIR, image_size=size,
                     small=True, features='image')

        img_bytes = df['image'].dropna().iloc[0]
        img = Image.open(BytesIO(img_bytes))

        size = size.lower()
        width, height = map(int, size.split('x'))

        assert img.size == (width, height)

    @pytest.mark.parametrize('format_', (
        'jpeg',
        'png',
    ))
    def test_image_format(self, format_):
        df = extract(SAMPLES_DIR, image_format=format_,
                     small=True, features='image')

        img_bytes = df['image'].dropna().iloc[0]
        img = Image.open(BytesIO(img_bytes))

        assert img.format.upper() == format_.upper()

    @pytest.mark.parametrize('ocr_image_size,is_low', (
        (200, True),
        (2000, False),
    ))
    def test_low_ocr_image(self, complete_df, ocr_image_size, is_low):
        df = extract_text(
            SAMPLES_DIR, small=True, ocr=True,
            ocr_image_size=ocr_image_size, ocr_lang='eng'
        )

        df = df.dropna(subset=['text'])
        serie = df.iloc[0]

        expected = complete_df.dropna(subset=['text'])
        expected = expected[(expected.path == serie.path)\
                            & (expected.page == serie.page)]

        expected_serie = expected.iloc[0]

        if is_low:
            assert serie.text.strip() != expected_serie.text.strip()
        else:
            assert serie.text.strip() == expected_serie.text.strip()

================================================
FILE: tests/test_extraction_memory.py
================================================
import pytest
import pandas as pd
from pdf2dataset import extract_text

from .testing_dataframe import check_and_compare
from .conftest import SAMPLES_DIR, PARQUET_ENGINE

@pytest.mark.parametrize('small', (
    True,
    False,
))
def test_passing_tasks(tmp_path, small):
    with open(SAMPLES_DIR / 'single_page1.pdf', 'rb') as f:
        pdf1_bin = f.read()

    with open(SAMPLES_DIR / 'multi_page1.pdf', 'rb') as f:
        pdf2_bin = f.read()

    tasks = [\
        ('doc1.pdf', pdf1_bin),  # All pages\
        ('2.pdf', pdf2_bin, 2),  # Just page 2\
        ('pdf2.pdf', pdf2_bin, 3),  # Just page 3\
    ]

    expected_dict = {
        'path': ['pdf2.pdf', '2.pdf', 'doc1.pdf'],
        'page': [3, 2, 1],
        'text': ['Third page', 'Second page', 'My beautiful sample!'],
        'error': [None, None, None],
    }
    expected = pd.DataFrame(expected_dict)

    if small:
        df = extract_text(tasks=tasks, small=small)
    else:
        result_path = tmp_path / 'result.parquet.gzip'
        extract_text(tasks, result_path)

        df = pd.read_parquet(result_path, engine=PARQUET_ENGINE)

    check_and_compare(df, expected, list(expected.columns))

================================================
FILE: tests/testing_dataframe.py
================================================
import copy
from hashlib import md5

class TestingDataFrame:
    def __init__(self, df):
        self._df = df
        self._df = self._df.fillna('').astype('str')

    def __repr__(self):
        columns = ', '.join(self._df.columns)
        values = str(self._df.values)

        return f'Columns: {columns}\n{values}'

    def _hash_images(self):
        if 'image' in self._df.columns:
            self._df['image'] = self._df['image'].apply(
                lambda c: md5(c.encode()).hexdigest() if c else None
            )

    def _pop_error_columns(self):
        for column in ('error', 'error_bool'):
            if column in self._df.columns:
                self._df.pop(column)

    def sort(self):
        columns = sorted(self._df.columns)

        self._df.sort_values(by=columns, inplace=True)
        self._df = self._df[columns]

    def check_errors(self, is_ocr):
        def check_error_msg(error):
            if error:
                assert 'Traceback' in error

                if 'text' in self._df.columns:
                    pdftotext_error_msg = 'poppler error creating document'
                    assert (pdftotext_error_msg in error) != is_ocr

        self._df['error'].apply(check_error_msg)

    def _assert_equal_errors(self, other):
        check_column = 'error'

        if check_column not in other._df.columns:
            self._df['error_bool'] = self._df.pop('error').apply(bool)
            check_column = 'error_bool'

        self_values = self._df[check_column].values
        other_values = other._df[check_column].values

        return (self_values == other_values).all()

    def assert_equal(self, expected):
        if any([self._df.empty, expected._df.empty]):
            if self._df.empty and expected._df.empty:
                assert True

            assert False

        self_cp = copy.deepcopy(self)
        expected_cp = copy.deepcopy(expected)

        self_cp._assert_equal_errors(expected_cp)
        self_cp._pop_error_columns()
        expected_cp._pop_error_columns()

        # Improve visualization for debugging
        self_cp._hash_images()
        expected_cp._hash_images()

        self_cp.sort()
        expected_cp.sort()

        # Making debug easier
        assert list(self_cp._df.columns) == list(expected_cp._df.columns)
        assert self_cp._df.shape == expected_cp._df.shape

        assert (self_cp._df.values == expected_cp._df.values).all()

    def check_and_compare(self, expected, is_ocr=False):
        self.check_errors(is_ocr)

        self.assert_equal(expected)

def check_and_compare(df, expected, is_ocr=False):
    expected = TestingDataFrame(expected)
    df = TestingDataFrame(df)

    df.check_and_compare(expected, is_ocr)

================================================
FILE: .github/workflows/ci.yml
================================================
name: pdf2dataset

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.6, 3.7, 3.8]

    steps:
    - name: Install Poetry
      uses: dschep/install-poetry-action@v1.3

    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        sudo apt update
        sudo apt install -y poppler-utils tesseract-ocr-por build-essential libpoppler-cpp-dev pkg-config python3-dev

        python -m pip install --upgrade pip
        poetry install

    - name: Lint with flake8
      run: |
        flake8 . --show-source --statistics

    - name: Test with pytest
      run: |
        pytest --cov=pdf2dataset

    - name: Codecov
      uses: codecov/codecov-action@v1.0.13
````